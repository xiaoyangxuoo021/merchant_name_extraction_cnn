{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"LOWER\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# spacy.download(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food and fried chicken\")\n",
    "token1 = doc1[2:4]\n",
    "token2 = doc1[5:6]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "# with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "#     COUNTRIES = json.loads(f.read())\n",
    "# with open(\"exercises/en/country_text.txt\", encoding=\"utf8\") as f:\n",
    "#     TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### method extension on the Doc or span objects, the first argument is equivalent to \"self\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom pipeline components with added doc attribute/method extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scleandataset = pd.read_csv('data/dataset.csv')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip the below two cells if you have shuffled_dataset in data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from random import shuffle\n",
    "\n",
    "shuffle_1 = cleandataset.copy()\n",
    "shuffle_2 = cleandataset.copy()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# shuffle_1['merchant_name'] = shuffle_1['merchant_name'].apply(lambda sentence: ' '.join(random.sample(sentence.split(' '), k = len(sentence.split(' ')))))\n",
    "# # shuffle_1.head()\n",
    "# shuffle_2['merchant_name'] = shuffle_2['merchant_name'].apply(lambda sentence: ' '.join(random.sample(sentence.split(' '), k = len(sentence.split(' ')))))\n",
    "# shuffle_2.sample(5)\n",
    "for idx, row in shuffle_1.iterrows():\n",
    "    if len(row[1].split()) > 1:\n",
    "        try:\n",
    "            sidx = row[0].find(row[1])\n",
    "            eidx = sidx + len(row[1])\n",
    "            merchant = row[1]\n",
    "            tempsentence = row[0][:]\n",
    "            insertion_pos = tempsentence.split().index(merchant.split()[0])\n",
    "            tempsentence_removed_tokenized = tempsentence.replace(row[1], ' ').split()\n",
    "            tempsentence_removed_tokenized.insert(insertion_pos, merchant)\n",
    "            shuffled_sentence = ' '.join(random.sample(tempsentence_removed_tokenized, k = len(tempsentence_removed_tokenized)))\n",
    "            shuffle_1.iloc[idx, 0] = shuffled_sentence\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        shuffle_1.iloc[idx, 0] = ' '.join(random.sample(row[0].split(' '), k = len(row[0].split(' '))))\n",
    "\n",
    "shuffle_1.reset_index(drop = True)\n",
    "for idx, row in shuffle_1.iterrows():\n",
    "    shuffle_1.iloc[idx, 2] = shuffle_1.iloc[idx, 0].find(shuffle_1.iloc[idx, 1])\n",
    "    shuffle_1.iloc[idx, 3] = shuffle_1.iloc[idx, 2] + len(shuffle_1.iloc[idx, 1])\n",
    "shuffle_1.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in shuffle_2.iterrows():\n",
    "    if len(row[1].split()) > 1:\n",
    "        try:\n",
    "            sidx = row[0].find(row[1])\n",
    "            eidx = sidx + len(row[1])\n",
    "            merchant = row[1]\n",
    "            tempsentence = row[0][:]\n",
    "            insertion_pos = tempsentence.split().index(merchant.split()[0])\n",
    "            tempsentence_removed_tokenized = tempsentence.replace(row[1], ' ').split()\n",
    "            tempsentence_removed_tokenized.insert(insertion_pos, merchant)\n",
    "            shuffled_sentence = ' '.join(random.sample(tempsentence_removed_tokenized, k = len(tempsentence_removed_tokenized)))\n",
    "            shuffle_2.iloc[idx, 0] = shuffled_sentence\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        shuffle_2.iloc[idx, 0] = ' '.join(random.sample(row[0].split(' '), k = len(row[0].split(' '))))\n",
    "\n",
    "shuffle_2.reset_index(drop = True)\n",
    "for idx, row in shuffle_2.iterrows():\n",
    "    shuffle_2.iloc[idx, 2] = shuffle_2.iloc[idx, 0].find(shuffle_2.iloc[idx, 1])\n",
    "    shuffle_2.iloc[idx, 3] = shuffle_2.iloc[idx, 2] + len(shuffle_2.iloc[idx, 1])\n",
    "shuffle_2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_temp = pd.concat([cleandataset, shuffle_1], axis = 0)\n",
    "result = pd.concat([result_temp, shuffle_2], axis = 0)\n",
    "result.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data/shuffled_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvb = pd.read_csv('data/mvb_cleanedmerchant_training_fourbanks.csv', on_bad_lines = 'warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.read_csv('data/shuffled_dataset.csv')\n",
    "if 'Unnamed: 0' in result.columns:\n",
    "    result.drop(columns=['Unnamed: 0'], inplace = True)\n",
    "    \n",
    "TRAIN_DATA = result.values.tolist()\n",
    "#TRAIN_DATA = [(item[0], {'merchant': item[1], 'entities':[(item[2], item[3], 'BRD')]}) for item in TRAIN_DATA]\n",
    "TRAIN_DATA = [(item[0], {'entities':[(item[2], item[3], 'BRD')]}) for item in TRAIN_DATA]\n",
    "TRAIN_DATA[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size = compounding(4, 32, 1.001))\n",
    "\n",
    "            for batch in batches:\n",
    "                text, annotations = zip(*batch)\n",
    "                # text = nlp.make_doc(text)    #<--- add this\n",
    "                # gold = GoldParse(text, entities=annotations)  #<--- add this\n",
    "                nlp.update(\n",
    "                    text,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    sgd = optimizer,  # callable to update weights\n",
    "                    drop = 0.35,\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "random.shuffle(TRAIN_DATA)\n",
    "\n",
    "train = TRAIN_DATA[:int(len(TRAIN_DATA) *0.07)]\n",
    "test = TRAIN_DATA[int(len(TRAIN_DATA) *0.07):]\n",
    "\n",
    "prdnlp = train_spacy(train, 30)\n",
    "# Save our trained Model\n",
    "modelfile = 'wtf_2'\n",
    "prdnlp.to_disk(modelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.require_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the trained pipeline into a .tar.gz package\n",
    "!python -m spacy package /path/to/output/model-best ./packages --name my_pipeline --version 1.0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generation of a  config\n",
    "!python -m spacy init config ./config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "nlp = spacy.blank(\"en\")\n",
    "doc3 = nlp(\"There's also a Paris in Arkansas, lol\")\n",
    "for ent in doc3.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TRAIN_DATA[int(len(TRAIN_DATA) *0.07):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test your text\n",
    "from fuzzywuzzy import fuzz\n",
    "test_text = sample(test, 1000)\n",
    "scores = []\n",
    "for text in test_text:\n",
    "    #print('test case: ', text)\n",
    "    doc = prdnlp(text[0])\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text)\n",
    "        # print('True label is: ')\n",
    "        # print(text[1]['merchant'])\n",
    "        similarity = fuzz.token_set_ratio(ent.text,text[1]['merchant'])\n",
    "        #print('similarity is: ', similarity)\n",
    "        scores.append(similarity)\n",
    "print(scores.count(100) / len(scores) *100, ' percent of test data has been perfectly correctly classified.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_nlp = spacy.load('wtf')\n",
    "test_text = sample(test, 10)\n",
    "for text in test_text:\n",
    "    print('test case: ', text)\n",
    "    doc = load_nlp(text[0])\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata = pd.read_csv('data/cleanedmerchant_training.csv') ## read the file into to a pandas dataframe\n",
    "data = tempdata[['merchant_name', 'cleanName2']]\n",
    "validName = data[data['cleanName2'].notnull()]\n",
    "validName.reset_index(inplace=True, drop = True)\n",
    "validName.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# mu, sigma = 100, 10 # mean and standard deviation\n",
    "# s = np.random.normal(mu, sigma, 1000)\n",
    "# s_2 = np.random.uniform(70, 130, 1000)\n",
    "\n",
    "# count, bins, ignored = plt.hist(s, 30, density=False, color = 'r')\n",
    "# # plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "# #                np.exp( - (bins - mu)**2 / (2 * sigma**2) )\n",
    "# #          )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "# \"They won't be very interesting, I'm afraid.\",\n",
    "# \"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "raw_docs = validName['merchant_name'].to_list()\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [' '.join(words) for words in tokenized_docs_no_stopwords]\n",
    "len(cleaned_sentences) == len(validName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName['merchant_name'] = cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle samples in dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: (list) list of tuples like: [(transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    Returns:\n",
    "        dataset: (list) list of tuples like: [,....., (transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = list(range(len(dataset)))\n",
    "    random.shuffle(shuffled_indices)\n",
    "    dataset = [dataset[index] for index in shuffled_indices]\n",
    "\n",
    "    return dataset\n",
    "def get_merchant_indices_in_sentence(sentence, merchant):\n",
    "    \"\"\"\n",
    "    Given transaction string and merchant string, returns start end end indices of merchant in transaction string.\n",
    "\n",
    "    Args:\n",
    "        sentence: (string) transaction string. (example: )\n",
    "        merchant: (string) merchant name. (example: )\n",
    "\n",
    "    Returns:\n",
    "        sentence: (string) converted to upper strings\n",
    "        start: (int) merchant string start index\n",
    "        end: (int) merchant string end index\n",
    "\n",
    "    Examples:\n",
    "        sentences, start, end = get_sentence_indices(\"Target 00014423 WATERTOWN MA\",\"target\")\n",
    "\n",
    "        sentences: \"TARGET 00014423 WATERTOWN MA\"\n",
    "        start: 0\n",
    "        end: 6\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.upper()\n",
    "    merchant = merchant.upper()\n",
    "\n",
    "    start = -1\n",
    "    end = -1\n",
    "\n",
    "    idx = sentence.find(merchant)\n",
    "    if idx != -1:\n",
    "        start = idx\n",
    "        end = start + len(merchant)\n",
    "\n",
    "    return sentence, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for idx, row in validName.iterrows():\n",
    "    # print('Processing row: ', idx)\n",
    "    # print('row0: ', row[0])\n",
    "    # print('row1: ', row[1])\n",
    "    # print(row[0].find(row[1].split(' ')[0]))\n",
    "    # print(len(row[1]))\n",
    "    find_i = row[0].find(row[1].split(' ')[0])\n",
    "    if find_i + len(row[1]) > len(row[0]):   \n",
    "        row[0] = row[0][:find_i] + row[1]\n",
    "    \n",
    "    sentence, start, end = get_merchant_indices_in_sentence(row[0], row[1])\n",
    "    validName.loc[idx, 'start_idx'] = start\n",
    "    validName.loc[idx, 'end_idx'] = end\n",
    "\n",
    "print(validName.sample(50))\n",
    "#validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset.drop(columns = 'Unnamed: 0')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset[cleandataset['start_idx']!=-1]\n",
    "len(cleandataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding(text, max_len=300, emb_dim=8):\n",
    "        \"\"\"\n",
    "        Embeds character string with the use of (emb_dim)-bit binary values of each character.\n",
    "\n",
    "        Args:\n",
    "            text: (string) text to embed\n",
    "            max_len: (int) maximum length of text that will be encoded. Padding will be done with zeros.\n",
    "            emb_dim:\n",
    "\n",
    "        Returns:\n",
    "            str_array: (ndarray) 2 dimensional numpy array containing embedded text of shape emb_dim*max_len\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # cut long text with maximum accepted length\n",
    "        if len(text) > max_len:\n",
    "            text = text[:max_len]\n",
    "\n",
    "        str_array = np.zeros((emb_dim, max(len(text), max_len)), dtype=np.int32).tolist()\n",
    "\n",
    "        for index, char in enumerate(text):\n",
    "            str_binary = format(ord(char), 'b').zfill(emb_dim)[::-1]\n",
    "            str_binary = str_binary[:emb_dim]\n",
    "            for str_index, str_char in enumerate(str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "       \n",
    "        padding_str_binary = '0' * emb_dim\n",
    "        \n",
    "        for index in range(len(text), max_len):\n",
    "            for str_index, str_char in enumerate(padding_str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "\n",
    "        return str_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "emb = character_embedding('Martin')\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleandataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df[:int(0.7 * len(df))]\n",
    "val_dataset = df[int(0.7 * len(df)): int(0.85 * len(df))]\n",
    "test_dataset = df[int(0.85 * len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"sID\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"dollar general w main a\")\n",
    "print(doc.ents)\n",
    "# for idx, row in sample_10.iterrows():\n",
    "#     # print('processing row: ', idx)\n",
    "#     # print(row[0])\n",
    "#     doc = nlp(row[0])\n",
    "#     all_entities = doc.ents\n",
    "#     print(all_entities)\n",
    "#     # print(all_entities)\n",
    "#     #extraction_temp = str(process.extractOne(row[1], all_entities)[0])\n",
    "#     #extraction =  re.sub(r\"inc|mktp|\\d+\", \"\", extraction_temp) \n",
    "#     #print('extraction result: ', extraction_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cleanData = pd.read_csv('data/cleanedmerchant_training.csv')\n",
    "cleanData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordFreeds[(stopwordFreeds['cleanName2'] != stopwordFreeds['CleanName3']) & (stopwordFreeds['CleanName3'] != 'chick fil')].sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
