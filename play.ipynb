{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['merchant_name', 'cleanName2', 'start_idx', 'end_idx'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandataset = pd.read_csv('data/dataset.csv')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  4, 16, 32, 26, 27, 39,  3,  9,  7,  8, 21,  2, 15, 23, 29, 20,\n",
       "       25, 22, 30, 24, 10,  6, 14, 31,  0, 13, 12,  1, 11, 17, 19, 34, 28,\n",
       "       33, 18, 35, 36, 37], dtype=int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandataset['start_idx'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0 04 amc creve coeur', {'entities': [(5, 8, 'BRD')]}),\n",
       " ('0 05 amc security sq', {'entities': [(5, 8, 'BRD')]}),\n",
       " ('0 1 forever 21', {'entities': [(4, 14, 'BRD')]}),\n",
       " ('0 2 forever 21', {'entities': [(4, 14, 'BRD')]}),\n",
       " ('0 3 amc grand island', {'entities': [(4, 7, 'BRD')]})]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = cleandataset.values.tolist()\n",
    "TRAIN_DATA = [(item[0], {'entities':[(item[2], item[3], 'BRD')]}) for item in TRAIN_DATA]\n",
    "TRAIN_DATA[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "{'ner': 679.0596929758201}\n",
      "Starting iteration 1\n",
      "{'ner': 347.8885497782279}\n",
      "Starting iteration 2\n",
      "{'ner': 218.77194210659124}\n",
      "Starting iteration 3\n",
      "{'ner': 182.31397867414307}\n",
      "Starting iteration 4\n",
      "{'ner': 168.8610008848184}\n",
      "Starting iteration 5\n",
      "{'ner': 167.533809333152}\n",
      "Starting iteration 6\n",
      "{'ner': 99.81080790196178}\n",
      "Starting iteration 7\n",
      "{'ner': 64.98547894595171}\n",
      "Starting iteration 8\n",
      "{'ner': 42.0636345749937}\n",
      "Starting iteration 9\n",
      "{'ner': 74.13608511613619}\n",
      "Starting iteration 10\n",
      "{'ner': 88.43316794159757}\n",
      "Starting iteration 11\n",
      "{'ner': 53.66041874493271}\n",
      "Starting iteration 12\n",
      "{'ner': 43.94755223043633}\n",
      "Starting iteration 13\n",
      "{'ner': 71.71231055686705}\n",
      "Starting iteration 14\n",
      "{'ner': 54.02206059437969}\n",
      "Starting iteration 15\n",
      "{'ner': 45.36082394173235}\n",
      "Starting iteration 16\n",
      "{'ner': 35.11150637072986}\n",
      "Starting iteration 17\n",
      "{'ner': 30.977853328374515}\n",
      "Starting iteration 18\n",
      "{'ner': 27.159490019437403}\n",
      "Starting iteration 19\n",
      "{'ner': 60.90726052075518}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "# add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "# get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "random.shuffle(TRAIN_DATA)\n",
    "train = TRAIN_DATA[:1000]\n",
    "# train = TRAIN_DATA[:int(len(TRAIN_DATA) *0.7)]\n",
    "# test = TRAIN_DATA[int(len(TRAIN_DATA) *0.7):]\n",
    "\n",
    "prdnlp = train_spacy(train, 20)\n",
    "# Save our trained Model\n",
    "modelfile = input(\"Enter your Model Name: \")\n",
    "prdnlp.to_disk(modelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case:  [('sheetz 0518 mechanicsburgpa usa', {'entities': [(0, 6, 'BRD')]})]\n",
      "sheetz\n"
     ]
    }
   ],
   "source": [
    "#Test your text\n",
    "test_text = sample(TRAIN_DATA, 1)\n",
    "\n",
    "print('test case: ', test_text)\n",
    "doc = prdnlp(test_text[0][0])\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata = pd.read_csv('data/cleanedmerchant_training.csv') ## read the file into to a pandas dataframe\n",
    "data = tempdata[['merchant_name', 'cleanName2']]\n",
    "validName = data[data['cleanName2'].notnull()]\n",
    "validName.reset_index(inplace=True, drop = True)\n",
    "validName.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# mu, sigma = 100, 10 # mean and standard deviation\n",
    "# s = np.random.normal(mu, sigma, 1000)\n",
    "# s_2 = np.random.uniform(70, 130, 1000)\n",
    "\n",
    "# count, bins, ignored = plt.hist(s, 30, density=False, color = 'r')\n",
    "# # plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "# #                np.exp( - (bins - mu)**2 / (2 * sigma**2) )\n",
    "# #          )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "# \"They won't be very interesting, I'm afraid.\",\n",
    "# \"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "raw_docs = validName['merchant_name'].to_list()\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [' '.join(words) for words in tokenized_docs_no_stopwords]\n",
    "len(cleaned_sentences) == len(validName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName['merchant_name'] = cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle samples in dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: (list) list of tuples like: [(transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    Returns:\n",
    "        dataset: (list) list of tuples like: [,....., (transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = list(range(len(dataset)))\n",
    "    random.shuffle(shuffled_indices)\n",
    "    dataset = [dataset[index] for index in shuffled_indices]\n",
    "\n",
    "    return dataset\n",
    "def get_merchant_indices_in_sentence(sentence, merchant):\n",
    "    \"\"\"\n",
    "    Given transaction string and merchant string, returns start end end indices of merchant in transaction string.\n",
    "\n",
    "    Args:\n",
    "        sentence: (string) transaction string. (example: )\n",
    "        merchant: (string) merchant name. (example: )\n",
    "\n",
    "    Returns:\n",
    "        sentence: (string) converted to upper strings\n",
    "        start: (int) merchant string start index\n",
    "        end: (int) merchant string end index\n",
    "\n",
    "    Examples:\n",
    "        sentences, start, end = get_sentence_indices(\"Target 00014423 WATERTOWN MA\",\"target\")\n",
    "\n",
    "        sentences: \"TARGET 00014423 WATERTOWN MA\"\n",
    "        start: 0\n",
    "        end: 6\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.upper()\n",
    "    merchant = merchant.upper()\n",
    "\n",
    "    start = -1\n",
    "    end = -1\n",
    "\n",
    "    idx = sentence.find(merchant)\n",
    "    if idx != -1:\n",
    "        start = idx\n",
    "        end = start + len(merchant)\n",
    "\n",
    "    return sentence, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for idx, row in validName.iterrows():\n",
    "    # print('Processing row: ', idx)\n",
    "    # print('row0: ', row[0])\n",
    "    # print('row1: ', row[1])\n",
    "    # print(row[0].find(row[1].split(' ')[0]))\n",
    "    # print(len(row[1]))\n",
    "    find_i = row[0].find(row[1].split(' ')[0])\n",
    "    if find_i + len(row[1]) > len(row[0]):   \n",
    "        row[0] = row[0][:find_i] + row[1]\n",
    "    \n",
    "    sentence, start, end = get_merchant_indices_in_sentence(row[0], row[1])\n",
    "    validName.loc[idx, 'start_idx'] = start\n",
    "    validName.loc[idx, 'end_idx'] = end\n",
    "\n",
    "print(validName.sample(50))\n",
    "#validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset.drop(columns = 'Unnamed: 0')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset[cleandataset['start_idx']!=-1]\n",
    "len(cleandataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding(text, max_len=300, emb_dim=8):\n",
    "        \"\"\"\n",
    "        Embeds character string with the use of (emb_dim)-bit binary values of each character.\n",
    "\n",
    "        Args:\n",
    "            text: (string) text to embed\n",
    "            max_len: (int) maximum length of text that will be encoded. Padding will be done with zeros.\n",
    "            emb_dim:\n",
    "\n",
    "        Returns:\n",
    "            str_array: (ndarray) 2 dimensional numpy array containing embedded text of shape emb_dim*max_len\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # cut long text with maximum accepted length\n",
    "        if len(text) > max_len:\n",
    "            text = text[:max_len]\n",
    "\n",
    "        str_array = np.zeros((emb_dim, max(len(text), max_len)), dtype=np.int32).tolist()\n",
    "\n",
    "        for index, char in enumerate(text):\n",
    "            str_binary = format(ord(char), 'b').zfill(emb_dim)[::-1]\n",
    "            str_binary = str_binary[:emb_dim]\n",
    "            for str_index, str_char in enumerate(str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "       \n",
    "        padding_str_binary = '0' * emb_dim\n",
    "        \n",
    "        for index in range(len(text), max_len):\n",
    "            for str_index, str_char in enumerate(padding_str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "\n",
    "        return str_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "emb = character_embedding('Martin')\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleandataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df[:int(0.7 * len(df))]\n",
    "val_dataset = df[int(0.7 * len(df)): int(0.85 * len(df))]\n",
    "test_dataset = df[int(0.85 * len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"sID\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"dollar general w main a\")\n",
    "print(doc.ents)\n",
    "# for idx, row in sample_10.iterrows():\n",
    "#     # print('processing row: ', idx)\n",
    "#     # print(row[0])\n",
    "#     doc = nlp(row[0])\n",
    "#     all_entities = doc.ents\n",
    "#     print(all_entities)\n",
    "#     # print(all_entities)\n",
    "#     #extraction_temp = str(process.extractOne(row[1], all_entities)[0])\n",
    "#     #extraction =  re.sub(r\"inc|mktp|\\d+\", \"\", extraction_temp) \n",
    "#     #print('extraction result: ', extraction_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cleanData = pd.read_csv('data/cleanedmerchant_training.csv')\n",
    "cleanData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordFreeds[(stopwordFreeds['cleanName2'] != stopwordFreeds['CleanName3']) & (stopwordFreeds['CleanName3'] != 'chick fil')].sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
