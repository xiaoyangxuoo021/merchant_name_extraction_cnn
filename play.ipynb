{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['merchant_name', 'cleanName2', 'start_idx', 'end_idx'], dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleandataset = pd.read_csv('data/dataset.csv')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>cleanName2</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>192022</th>\n",
       "      <td>fu shelby tn 1230 kroger memphis</td>\n",
       "      <td>kroger</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182031</th>\n",
       "      <td>u 991597806779 hulu hlu</td>\n",
       "      <td>hulu</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220663</th>\n",
       "      <td>susansmith7 ebay paypal</td>\n",
       "      <td>ebay</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10515</th>\n",
       "      <td>0y82133n3 amazon amzn</td>\n",
       "      <td>amazon</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140890</th>\n",
       "      <td>parris b dollar general</td>\n",
       "      <td>dollar general</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           merchant_name      cleanName2  start_idx  end_idx\n",
       "192022  fu shelby tn 1230 kroger memphis          kroger         18       24\n",
       "182031           u 991597806779 hulu hlu            hulu         15       19\n",
       "220663           susansmith7 ebay paypal            ebay         12       16\n",
       "10515              0y82133n3 amazon amzn          amazon         10       16\n",
       "140890           parris b dollar general  dollar general          9       23"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from random import shuffle\n",
    "\n",
    "shuffle_1 = cleandataset.copy()\n",
    "shuffle_2 = cleandataset.copy()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# shuffle_1['merchant_name'] = shuffle_1['merchant_name'].apply(lambda sentence: ' '.join(random.sample(sentence.split(' '), k = len(sentence.split(' ')))))\n",
    "# # shuffle_1.head()\n",
    "# shuffle_2['merchant_name'] = shuffle_2['merchant_name'].apply(lambda sentence: ' '.join(random.sample(sentence.split(' '), k = len(sentence.split(' ')))))\n",
    "# shuffle_2.sample(5)\n",
    "for idx, row in shuffle_1.iterrows():\n",
    "    if len(row[1].split()) > 1:\n",
    "        try:\n",
    "            sidx = row[0].find(row[1])\n",
    "            eidx = sidx + len(row[1])\n",
    "            merchant = row[1]\n",
    "            tempsentence = row[0][:]\n",
    "            insertion_pos = tempsentence.split().index(merchant.split()[0])\n",
    "            tempsentence_removed_tokenized = tempsentence.replace(row[1], ' ').split()\n",
    "            tempsentence_removed_tokenized.insert(insertion_pos, merchant)\n",
    "            shuffled_sentence = ' '.join(random.sample(tempsentence_removed_tokenized, k = len(tempsentence_removed_tokenized)))\n",
    "            shuffle_1.iloc[idx, 0] = shuffled_sentence\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        shuffle_1.iloc[idx, 0] = ' '.join(random.sample(row[0].split(' '), k = len(row[0].split(' '))))\n",
    "\n",
    "shuffle_1.reset_index(drop = True)\n",
    "for idx, row in shuffle_1.iterrows():\n",
    "    shuffle_1.iloc[idx, 2] = shuffle_1.iloc[idx, 0].find(shuffle_1.iloc[idx, 1])\n",
    "    shuffle_1.iloc[idx, 3] = shuffle_1.iloc[idx, 2] + len(shuffle_1.iloc[idx, 1])\n",
    "shuffle_1.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>cleanName2</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229208</th>\n",
       "      <td>garden fe449 postmates</td>\n",
       "      <td>postmates</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79113</th>\n",
       "      <td>ys55r6hb amazon</td>\n",
       "      <td>amazon</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189931</th>\n",
       "      <td>030 washing 5312 kohls</td>\n",
       "      <td>kohls</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175016</th>\n",
       "      <td>u hulu hlu 1560226836583</td>\n",
       "      <td>hulu</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81910</th>\n",
       "      <td>chick cincinnati aramark oh usa fil</td>\n",
       "      <td>aramark</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              merchant_name cleanName2  start_idx  end_idx\n",
       "229208               garden fe449 postmates  postmates         13       22\n",
       "79113                       ys55r6hb amazon     amazon          9       15\n",
       "189931               030 washing 5312 kohls      kohls         17       22\n",
       "175016             u hulu hlu 1560226836583       hulu          2        6\n",
       "81910   chick cincinnati aramark oh usa fil    aramark         17       24"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for idx, row in shuffle_2.iterrows():\n",
    "    if len(row[1].split()) > 1:\n",
    "        try:\n",
    "            sidx = row[0].find(row[1])\n",
    "            eidx = sidx + len(row[1])\n",
    "            merchant = row[1]\n",
    "            tempsentence = row[0][:]\n",
    "            insertion_pos = tempsentence.split().index(merchant.split()[0])\n",
    "            tempsentence_removed_tokenized = tempsentence.replace(row[1], ' ').split()\n",
    "            tempsentence_removed_tokenized.insert(insertion_pos, merchant)\n",
    "            shuffled_sentence = ' '.join(random.sample(tempsentence_removed_tokenized, k = len(tempsentence_removed_tokenized)))\n",
    "            shuffle_2.iloc[idx, 0] = shuffled_sentence\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        shuffle_2.iloc[idx, 0] = ' '.join(random.sample(row[0].split(' '), k = len(row[0].split(' '))))\n",
    "\n",
    "shuffle_2.reset_index(drop = True)\n",
    "for idx, row in shuffle_2.iterrows():\n",
    "    shuffle_2.iloc[idx, 2] = shuffle_2.iloc[idx, 0].find(shuffle_2.iloc[idx, 1])\n",
    "    shuffle_2.iloc[idx, 3] = shuffle_2.iloc[idx, 2] + len(shuffle_2.iloc[idx, 1])\n",
    "shuffle_2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>cleanName2</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>262746</th>\n",
       "      <td>parker co target usa</td>\n",
       "      <td>target</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280977</th>\n",
       "      <td>r wegmans ridge</td>\n",
       "      <td>wegmans</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147019</th>\n",
       "      <td>doordash slim chickens</td>\n",
       "      <td>doordash</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108390</th>\n",
       "      <td>jen cash app melvin</td>\n",
       "      <td>cash app</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122411</th>\n",
       "      <td>gas sunny citgo</td>\n",
       "      <td>citgo</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 merchant_name cleanName2  start_idx  end_idx\n",
       "262746    parker co target usa     target         10       16\n",
       "280977         r wegmans ridge    wegmans          2        9\n",
       "147019  doordash slim chickens   doordash          0        8\n",
       "108390     jen cash app melvin   cash app          4       12\n",
       "122411         gas sunny citgo      citgo         10       15"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_temp = pd.concat([cleandataset, shuffle_1], axis = 0)\n",
    "result = pd.concat([result_temp, shuffle_2], axis = 0)\n",
    "result.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>cleanName2</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0 04 amc creve coeur</td>\n",
       "      <td>amc</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0 05 amc security sq</td>\n",
       "      <td>amc</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0 1 forever 21</td>\n",
       "      <td>forever 21</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0 2 forever 21</td>\n",
       "      <td>forever 21</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0 3 amc grand island</td>\n",
       "      <td>amc</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863056</th>\n",
       "      <td>bp zw100082 tn w100082 jackson</td>\n",
       "      <td>bp</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863057</th>\n",
       "      <td>new zxbuzamxgxnwnst york city membership n pel...</td>\n",
       "      <td>peloton</td>\n",
       "      <td>43</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863058</th>\n",
       "      <td>llc zyia active</td>\n",
       "      <td>zyia active</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863059</th>\n",
       "      <td>zyia active llc usa 4353835263 ut</td>\n",
       "      <td>zyia active</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863060</th>\n",
       "      <td>z zounds</td>\n",
       "      <td>z zounds</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>863061 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            merchant_name   cleanName2  \\\n",
       "0                                    0 04 amc creve coeur          amc   \n",
       "1                                    0 05 amc security sq          amc   \n",
       "2                                          0 1 forever 21   forever 21   \n",
       "3                                          0 2 forever 21   forever 21   \n",
       "4                                    0 3 amc grand island          amc   \n",
       "...                                                   ...          ...   \n",
       "863056                     bp zw100082 tn w100082 jackson           bp   \n",
       "863057  new zxbuzamxgxnwnst york city membership n pel...      peloton   \n",
       "863058                                    llc zyia active  zyia active   \n",
       "863059                  zyia active llc usa 4353835263 ut  zyia active   \n",
       "863060                                           z zounds     z zounds   \n",
       "\n",
       "        start_idx  end_idx  \n",
       "0               5        8  \n",
       "1               5        8  \n",
       "2               4       14  \n",
       "3               4       14  \n",
       "4               4        7  \n",
       "...           ...      ...  \n",
       "863056          0        2  \n",
       "863057         43       50  \n",
       "863058          4       15  \n",
       "863059          0       11  \n",
       "863060          0        8  \n",
       "\n",
       "[863061 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data/shuffled_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60414.200000000004"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(result) /\n",
    "863060*0.07\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0 04 amc creve coeur', {'merchant': 'amc', 'entities': [(5, 8, 'BRD')]}),\n",
       " ('0 05 amc security sq', {'merchant': 'amc', 'entities': [(5, 8, 'BRD')]}),\n",
       " ('0 1 forever 21', {'merchant': 'forever 21', 'entities': [(4, 14, 'BRD')]}),\n",
       " ('0 2 forever 21', {'merchant': 'forever 21', 'entities': [(4, 14, 'BRD')]}),\n",
       " ('0 3 amc grand island', {'merchant': 'amc', 'entities': [(4, 7, 'BRD')]})]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TRAIN_DATA = result.values.tolist()\n",
    "TRAIN_DATA = [(item[0], {'merchant': item[1], 'entities':[(item[2], item[3], 'BRD')]}) for item in TRAIN_DATA]\n",
    "TRAIN_DATA[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n",
      "{'ner': 14785.280735623952}\n",
      "Starting iteration 1\n",
      "{'ner': 8050.384688175385}\n",
      "Starting iteration 2\n",
      "{'ner': 7032.637879721816}\n",
      "Starting iteration 3\n",
      "{'ner': 6820.436508659606}\n",
      "Starting iteration 4\n",
      "{'ner': 6619.630067326758}\n",
      "Starting iteration 5\n",
      "{'ner': 6850.296819111672}\n",
      "Starting iteration 6\n",
      "{'ner': 6620.3372691328905}\n",
      "Starting iteration 7\n",
      "{'ner': 6467.140071386397}\n",
      "Starting iteration 8\n",
      "{'ner': 6485.079976149564}\n",
      "Starting iteration 9\n",
      "{'ner': 6599.898024007217}\n",
      "Starting iteration 10\n",
      "{'ner': 6824.941364560558}\n",
      "Starting iteration 11\n",
      "{'ner': 6730.289790247451}\n",
      "Starting iteration 12\n",
      "{'ner': 6704.052421643572}\n",
      "Starting iteration 13\n",
      "{'ner': 6599.488980906231}\n",
      "Starting iteration 14\n",
      "{'ner': 6556.489141455509}\n",
      "Starting iteration 15\n",
      "{'ner': 6469.259250730547}\n",
      "Starting iteration 16\n",
      "{'ner': 6464.355498739713}\n",
      "Starting iteration 17\n",
      "{'ner': 6448.337810457536}\n",
      "Starting iteration 18\n",
      "{'ner': 6697.459910993406}\n",
      "Starting iteration 19\n",
      "{'ner': 6531.123988932317}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "# add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "# get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            for text, annotations in TRAIN_DATA:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "random.shuffle(TRAIN_DATA)\n",
    "\n",
    "train = TRAIN_DATA[:int(len(TRAIN_DATA) *0.07)]\n",
    "test = TRAIN_DATA[int(len(TRAIN_DATA) *0.07):]\n",
    "\n",
    "prdnlp = train_spacy(train, 20)\n",
    "# Save our trained Model\n",
    "modelfile = 'wtf'\n",
    "prdnlp.to_disk(modelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = TRAIN_DATA[int(len(TRAIN_DATA) *0.07):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95.1  percent of test data has been correctly classified.\n"
     ]
    }
   ],
   "source": [
    "#Test your text\n",
    "from fuzzywuzzy import fuzz\n",
    "test_text = sample(test, 1000)\n",
    "scores = []\n",
    "for text in test_text:\n",
    "    #print('test case: ', text)\n",
    "    doc = prdnlp(text[0])\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text)\n",
    "        # print('True label is: ')\n",
    "        # print(text[1]['merchant'])\n",
    "        similarity = fuzz.token_sort_ratio(ent.text,text[1]['merchant'])\n",
    "        #print('similarity is: ', similarity)\n",
    "        scores.append(similarity)\n",
    "print(scores.count(100) / len(scores) *100, ' percent of test data has been correctly classified.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case:  ('paypal claudetebah ebay', {'entities': [(19, 23, 'BRD')]})\n",
      "ebay\n",
      "test case:  ('healthful essence', {'entities': [(0, 17, 'BRD')]})\n",
      "healthful essence\n",
      "test case:  ('taco bell ca murrieta qps', {'entities': [(0, 9, 'BRD')]})\n",
      "taco bell\n",
      "test case:  ('amazon amzn 764lv36i3', {'entities': [(0, 6, 'BRD')]})\n",
      "amazon\n",
      "test case:  ('pla dollar general', {'entities': [(4, 18, 'BRD')]})\n",
      "dollar general\n",
      "test case:  ('amazon dk5h92da3 seattle wa usa', {'entities': [(0, 6, 'BRD')]})\n",
      "amazon\n",
      "test case:  ('taqueria red 3 hermanos', {'entities': [(13, 23, 'BRD')]})\n",
      "red 3\n",
      "test case:  ('rj64g2qr amazon', {'entities': [(9, 15, 'BRD')]})\n",
      "amazon\n",
      "test case:  ('citgo greenf', {'entities': [(0, 5, 'BRD')]})\n",
      "citgo\n",
      "test case:  ('amazon c43nw12l', {'entities': [(0, 6, 'BRD')]})\n",
      "amazon\n"
     ]
    }
   ],
   "source": [
    "load_nlp = spacy.load('wtf')\n",
    "test_text = sample(test, 10)\n",
    "for text in test_text:\n",
    "    print('test case: ', text)\n",
    "    doc = load_nlp(text[0])\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tempdata = pd.read_csv('data/cleanedmerchant_training.csv') ## read the file into to a pandas dataframe\n",
    "data = tempdata[['merchant_name', 'cleanName2']]\n",
    "validName = data[data['cleanName2'].notnull()]\n",
    "validName.reset_index(inplace=True, drop = True)\n",
    "validName.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# mu, sigma = 100, 10 # mean and standard deviation\n",
    "# s = np.random.normal(mu, sigma, 1000)\n",
    "# s_2 = np.random.uniform(70, 130, 1000)\n",
    "\n",
    "# count, bins, ignored = plt.hist(s, 30, density=False, color = 'r')\n",
    "# # plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "# #                np.exp( - (bins - mu)**2 / (2 * sigma**2) )\n",
    "# #          )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "# \"They won't be very interesting, I'm afraid.\",\n",
    "# \"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "raw_docs = validName['merchant_name'].to_list()\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [' '.join(words) for words in tokenized_docs_no_stopwords]\n",
    "len(cleaned_sentences) == len(validName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName['merchant_name'] = cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle samples in dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: (list) list of tuples like: [(transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    Returns:\n",
    "        dataset: (list) list of tuples like: [,....., (transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = list(range(len(dataset)))\n",
    "    random.shuffle(shuffled_indices)\n",
    "    dataset = [dataset[index] for index in shuffled_indices]\n",
    "\n",
    "    return dataset\n",
    "def get_merchant_indices_in_sentence(sentence, merchant):\n",
    "    \"\"\"\n",
    "    Given transaction string and merchant string, returns start end end indices of merchant in transaction string.\n",
    "\n",
    "    Args:\n",
    "        sentence: (string) transaction string. (example: )\n",
    "        merchant: (string) merchant name. (example: )\n",
    "\n",
    "    Returns:\n",
    "        sentence: (string) converted to upper strings\n",
    "        start: (int) merchant string start index\n",
    "        end: (int) merchant string end index\n",
    "\n",
    "    Examples:\n",
    "        sentences, start, end = get_sentence_indices(\"Target 00014423 WATERTOWN MA\",\"target\")\n",
    "\n",
    "        sentences: \"TARGET 00014423 WATERTOWN MA\"\n",
    "        start: 0\n",
    "        end: 6\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.upper()\n",
    "    merchant = merchant.upper()\n",
    "\n",
    "    start = -1\n",
    "    end = -1\n",
    "\n",
    "    idx = sentence.find(merchant)\n",
    "    if idx != -1:\n",
    "        start = idx\n",
    "        end = start + len(merchant)\n",
    "\n",
    "    return sentence, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for idx, row in validName.iterrows():\n",
    "    # print('Processing row: ', idx)\n",
    "    # print('row0: ', row[0])\n",
    "    # print('row1: ', row[1])\n",
    "    # print(row[0].find(row[1].split(' ')[0]))\n",
    "    # print(len(row[1]))\n",
    "    find_i = row[0].find(row[1].split(' ')[0])\n",
    "    if find_i + len(row[1]) > len(row[0]):   \n",
    "        row[0] = row[0][:find_i] + row[1]\n",
    "    \n",
    "    sentence, start, end = get_merchant_indices_in_sentence(row[0], row[1])\n",
    "    validName.loc[idx, 'start_idx'] = start\n",
    "    validName.loc[idx, 'end_idx'] = end\n",
    "\n",
    "print(validName.sample(50))\n",
    "#validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset.drop(columns = 'Unnamed: 0')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset[cleandataset['start_idx']!=-1]\n",
    "len(cleandataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding(text, max_len=300, emb_dim=8):\n",
    "        \"\"\"\n",
    "        Embeds character string with the use of (emb_dim)-bit binary values of each character.\n",
    "\n",
    "        Args:\n",
    "            text: (string) text to embed\n",
    "            max_len: (int) maximum length of text that will be encoded. Padding will be done with zeros.\n",
    "            emb_dim:\n",
    "\n",
    "        Returns:\n",
    "            str_array: (ndarray) 2 dimensional numpy array containing embedded text of shape emb_dim*max_len\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # cut long text with maximum accepted length\n",
    "        if len(text) > max_len:\n",
    "            text = text[:max_len]\n",
    "\n",
    "        str_array = np.zeros((emb_dim, max(len(text), max_len)), dtype=np.int32).tolist()\n",
    "\n",
    "        for index, char in enumerate(text):\n",
    "            str_binary = format(ord(char), 'b').zfill(emb_dim)[::-1]\n",
    "            str_binary = str_binary[:emb_dim]\n",
    "            for str_index, str_char in enumerate(str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "       \n",
    "        padding_str_binary = '0' * emb_dim\n",
    "        \n",
    "        for index in range(len(text), max_len):\n",
    "            for str_index, str_char in enumerate(padding_str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "\n",
    "        return str_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "emb = character_embedding('Martin')\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleandataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df[:int(0.7 * len(df))]\n",
    "val_dataset = df[int(0.7 * len(df)): int(0.85 * len(df))]\n",
    "test_dataset = df[int(0.85 * len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"sID\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"dollar general w main a\")\n",
    "print(doc.ents)\n",
    "# for idx, row in sample_10.iterrows():\n",
    "#     # print('processing row: ', idx)\n",
    "#     # print(row[0])\n",
    "#     doc = nlp(row[0])\n",
    "#     all_entities = doc.ents\n",
    "#     print(all_entities)\n",
    "#     # print(all_entities)\n",
    "#     #extraction_temp = str(process.extractOne(row[1], all_entities)[0])\n",
    "#     #extraction =  re.sub(r\"inc|mktp|\\d+\", \"\", extraction_temp) \n",
    "#     #print('extraction result: ', extraction_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cleanData = pd.read_csv('data/cleanedmerchant_training.csv')\n",
    "cleanData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordFreeds[(stopwordFreeds['cleanName2'] != stopwordFreeds['CleanName3']) & (stopwordFreeds['CleanName3'] != 'chick fil')].sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
