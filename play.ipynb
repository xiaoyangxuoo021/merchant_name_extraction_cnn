{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a pipeline and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "pattern = [{\"TEXT\": \"iPhone\"}, {\"LOWER\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", [pattern])\n",
    "\n",
    "# Process some text\n",
    "doc = nlp(\"Upcoming iPhone X release date leaked\")\n",
    "\n",
    "# Call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.vocab.strings.add(\"coffee\")\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# Create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# Create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "\n",
    "# Create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"GREETING\")\n",
    "\n",
    "# Add span to the doc.ents\n",
    "doc.ents = [span_with_label]\n",
    "for ent in doc.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "# spacy.download(\"en_core_web_md\")\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food and fried chicken\")\n",
    "token1 = doc1[2:4]\n",
    "token2 = doc1[5:6]\n",
    "\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "import json\n",
    "\n",
    "# with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "#     COUNTRIES = json.loads(f.read())\n",
    "# with open(\"exercises/en/country_text.txt\", encoding=\"utf8\") as f:\n",
    "#     TEXT = f.read()\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "patterns = list(nlp.pipe(COUNTRIES))\n",
    "matcher.add(\"COUNTRY\", patterns)\n",
    "\n",
    "# Create a doc and reset existing entities\n",
    "doc = nlp(TEXT)\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "\n",
    "    # Overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents) + [span]\n",
    "\n",
    "    # Get the span's root head token\n",
    "    span_root_head = span.root.head\n",
    "    # Print the text of the span root's head token and the span text\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "\n",
    "# Print the entities in the document\n",
    "print([(ent.text, ent.label_) for ent in doc.ents if ent.label_ == \"GPE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    \"Twitch Prime, the perks program for Amazon Prime members offering free \"\n",
    "    \"loot, games and other benefits, is ditching one of its best features: \"\n",
    "    \"ad-free viewing. According to an email sent out to Amazon Prime members \"\n",
    "    \"today, ad-free viewing will no longer be included as a part of Twitch \"\n",
    "    \"Prime for new members, beginning on September 14. However, members with \"\n",
    "    \"existing annual subscriptions will be able to continue to enjoy ad-free \"\n",
    "    \"viewing until their subscription comes up for renewal. Those with \"\n",
    "    \"monthly subscriptions will have access to ad-free viewing until October 15.\"\n",
    ")\n",
    "\n",
    "# Create the match patterns\n",
    "pattern1 = [{\"LOWER\": \"amazon\"}, {\"POS\": \"PROPN\"}]\n",
    "pattern2 = [{\"LOWER\": \"ad\"}, {\"TEXT\": \"-\"}, {\"LOWER\": \"free\"}, {\"POS\": \"NOUN\"}]\n",
    "\n",
    "# Initialize the Matcher and add the patterns\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PATTERN1\", [pattern1])\n",
    "matcher.add(\"PATTERN2\", [pattern2])\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Print pattern string name and text of matched span\n",
    "    print(doc.vocab.strings[match_id], doc[start:end].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### method extension on the Doc or span objects, the first argument is equivalent to \"self\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Doc\n",
    "\n",
    "# Define method with arguments\n",
    "def has_token(doc, token_text):\n",
    "    in_doc = token_text in [token.text for token in doc]\n",
    "    return in_doc\n",
    "\n",
    "# Set extension on the Doc with method\n",
    "Doc.set_extension(\"has_token\", method=has_token)\n",
    "\n",
    "doc = nlp(\"The sky is blue.\")\n",
    "print(doc._.has_token(\"blue\"), \"- blue\")\n",
    "print(doc._.has_token(\"cloud\"), \"- cloud\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### custom pipeline components with added doc attribute/method extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "with open(\"exercises/en/countries.json\", encoding=\"utf8\") as f:\n",
    "    COUNTRIES = json.loads(f.read())\n",
    "\n",
    "with open(\"exercises/en/capitals.json\", encoding=\"utf8\") as f:\n",
    "    CAPITALS = json.loads(f.read())\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add(\"COUNTRY\", list(nlp.pipe(COUNTRIES)))\n",
    "\n",
    "\n",
    "@Language.component(\"countries_component\")\n",
    "def countries_component_function(doc):\n",
    "    # Create an entity Span with the label \"GPE\" for all matches\n",
    "    matches = matcher(doc)\n",
    "    doc.ents = [Span(doc, start, end, label=\"GPE\") for match_id, start, end in matches]\n",
    "    return doc\n",
    "\n",
    "\n",
    "# Add the component to the pipeline\n",
    "nlp.add_pipe(\"countries_component\")\n",
    "print(nlp.pipe_names)\n",
    "\n",
    "# Getter that looks up the span text in the dictionary of country capitals\n",
    "get_capital = lambda span: CAPITALS.get(span.text)\n",
    "\n",
    "# Register the Span extension attribute \"capital\" with the getter get_capital\n",
    "Span.set_extension(\"capital\", getter=get_capital)\n",
    "\n",
    "# Process the text and print the entity text, label and capital attributes\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "print([(ent.text, ent.label_, ent._.capital) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scleandataset = pd.read_csv('data/dataset.csv')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skip the below two cells if you have shuffled_dataset in data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from random import shuffle\n",
    "\n",
    "shuffle_1 = cleandataset.copy()\n",
    "shuffle_2 = cleandataset.copy()\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "# shuffle_1['merchant_name'] = shuffle_1['merchant_name'].apply(lambda sentence: ' '.join(random.sample(sentence.split(' '), k = len(sentence.split(' ')))))\n",
    "# # shuffle_1.head()\n",
    "# shuffle_2['merchant_name'] = shuffle_2['merchant_name'].apply(lambda sentence: ' '.join(random.sample(sentence.split(' '), k = len(sentence.split(' ')))))\n",
    "# shuffle_2.sample(5)\n",
    "for idx, row in shuffle_1.iterrows():\n",
    "    if len(row[1].split()) > 1:\n",
    "        try:\n",
    "            sidx = row[0].find(row[1])\n",
    "            eidx = sidx + len(row[1])\n",
    "            merchant = row[1]\n",
    "            tempsentence = row[0][:]\n",
    "            insertion_pos = tempsentence.split().index(merchant.split()[0])\n",
    "            tempsentence_removed_tokenized = tempsentence.replace(row[1], ' ').split()\n",
    "            tempsentence_removed_tokenized.insert(insertion_pos, merchant)\n",
    "            shuffled_sentence = ' '.join(random.sample(tempsentence_removed_tokenized, k = len(tempsentence_removed_tokenized)))\n",
    "            shuffle_1.iloc[idx, 0] = shuffled_sentence\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        shuffle_1.iloc[idx, 0] = ' '.join(random.sample(row[0].split(' '), k = len(row[0].split(' '))))\n",
    "\n",
    "shuffle_1.reset_index(drop = True)\n",
    "for idx, row in shuffle_1.iterrows():\n",
    "    shuffle_1.iloc[idx, 2] = shuffle_1.iloc[idx, 0].find(shuffle_1.iloc[idx, 1])\n",
    "    shuffle_1.iloc[idx, 3] = shuffle_1.iloc[idx, 2] + len(shuffle_1.iloc[idx, 1])\n",
    "shuffle_1.sample(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in shuffle_2.iterrows():\n",
    "    if len(row[1].split()) > 1:\n",
    "        try:\n",
    "            sidx = row[0].find(row[1])\n",
    "            eidx = sidx + len(row[1])\n",
    "            merchant = row[1]\n",
    "            tempsentence = row[0][:]\n",
    "            insertion_pos = tempsentence.split().index(merchant.split()[0])\n",
    "            tempsentence_removed_tokenized = tempsentence.replace(row[1], ' ').split()\n",
    "            tempsentence_removed_tokenized.insert(insertion_pos, merchant)\n",
    "            shuffled_sentence = ' '.join(random.sample(tempsentence_removed_tokenized, k = len(tempsentence_removed_tokenized)))\n",
    "            shuffle_2.iloc[idx, 0] = shuffled_sentence\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "    else:\n",
    "        shuffle_2.iloc[idx, 0] = ' '.join(random.sample(row[0].split(' '), k = len(row[0].split(' '))))\n",
    "\n",
    "shuffle_2.reset_index(drop = True)\n",
    "for idx, row in shuffle_2.iterrows():\n",
    "    shuffle_2.iloc[idx, 2] = shuffle_2.iloc[idx, 0].find(shuffle_2.iloc[idx, 1])\n",
    "    shuffle_2.iloc[idx, 3] = shuffle_2.iloc[idx, 2] + len(shuffle_2.iloc[idx, 1])\n",
    "shuffle_2.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_temp = pd.concat([cleandataset, shuffle_1], axis = 0)\n",
    "result = pd.concat([result_temp, shuffle_2], axis = 0)\n",
    "result.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data/shuffled_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvb = pd.read_csv('data/mvb_cleanedmerchant_training_fourbanks.csv', on_bad_lines = 'warn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mvb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0 04 amc creve coeur', {'entities': [(5, 8, 'BRD')]}),\n",
       " ('0 05 amc security sq', {'entities': [(5, 8, 'BRD')]}),\n",
       " ('0 1 forever 21', {'entities': [(4, 14, 'BRD')]}),\n",
       " ('0 2 forever 21', {'entities': [(4, 14, 'BRD')]}),\n",
       " ('0 3 amc grand island', {'entities': [(4, 7, 'BRD')]})]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.read_csv('data/shuffled_dataset.csv')\n",
    "if 'Unnamed: 0' in result.columns:\n",
    "    result.drop(columns=['Unnamed: 0'], inplace = True)\n",
    "    \n",
    "TRAIN_DATA = result.values.tolist()\n",
    "#TRAIN_DATA = [(item[0], {'merchant': item[1], 'entities':[(item[2], item[3], 'BRD')]}) for item in TRAIN_DATA]\n",
    "TRAIN_DATA = [(item[0], {'entities':[(item[2], item[3], 'BRD')]}) for item in TRAIN_DATA]\n",
    "TRAIN_DATA[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>cleanName2</th>\n",
       "      <th>start_idx</th>\n",
       "      <th>end_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>621186</th>\n",
       "      <td>amazon amzn my0v78sl0</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>808965</th>\n",
       "      <td>paus 657936166201 800 3of5 9444 qvc</td>\n",
       "      <td>qvc</td>\n",
       "      <td>32</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109754</th>\n",
       "      <td>cash app yvvyv add 8774174551 caus</td>\n",
       "      <td>cash app</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382732</th>\n",
       "      <td>countrysiqps bp</td>\n",
       "      <td>bp</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639512</th>\n",
       "      <td>ms8dy6th prime amazon</td>\n",
       "      <td>amazon</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>463779</th>\n",
       "      <td>hlu hulu u 1615027663373</td>\n",
       "      <td>hulu</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443750</th>\n",
       "      <td>park five star orange</td>\n",
       "      <td>five star</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>359117</th>\n",
       "      <td>amazon prime zc6558yg</td>\n",
       "      <td>amazon</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732754</th>\n",
       "      <td>aftwvx ai frontier</td>\n",
       "      <td>frontier</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>814345</th>\n",
       "      <td>scnb</td>\n",
       "      <td>scnb</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              merchant_name cleanName2  start_idx  end_idx\n",
       "621186                amazon amzn my0v78sl0     amazon          0        6\n",
       "808965  paus 657936166201 800 3of5 9444 qvc        qvc         32       35\n",
       "109754   cash app yvvyv add 8774174551 caus   cash app          0        8\n",
       "382732                      countrysiqps bp         bp         13       15\n",
       "639512                ms8dy6th prime amazon     amazon         15       21\n",
       "463779             hlu hulu u 1615027663373       hulu          4        8\n",
       "443750                park five star orange  five star          5       14\n",
       "359117                amazon prime zc6558yg     amazon          0        6\n",
       "732754                   aftwvx ai frontier   frontier         10       18\n",
       "814345                                 scnb       scnb          0        4"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting iteration 0\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "from random import sample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spacy.util import minibatch, compounding\n",
    "from spacy.gold import GoldParse\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def train_spacy(data,iterations):\n",
    "    TRAIN_DATA = data\n",
    "    nlp = spacy.blank('en')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "    # add labels\n",
    "    for _, annotations in TRAIN_DATA:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Starting iteration \" + str(itn))\n",
    "            random.shuffle(TRAIN_DATA)\n",
    "            losses = {}\n",
    "            batches = minibatch(TRAIN_DATA, size = compounding(4, 32, 1.001))\n",
    "            for batch in batches:\n",
    "                text, annotations = zip(*batch)\n",
    "                # text = nlp.make_doc(text)    #<--- add this\n",
    "                # gold = GoldParse(text, entities=annotations)  #<--- add this\n",
    "                nlp.update(\n",
    "                    text,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    sgd = optimizer,  # callable to update weights\n",
    "                    drop = 0.35,\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "random.shuffle(TRAIN_DATA)\n",
    "\n",
    "train = TRAIN_DATA[:int(len(TRAIN_DATA) *0.07)]\n",
    "test = TRAIN_DATA[int(len(TRAIN_DATA) *0.07):]\n",
    "\n",
    "prdnlp = train_spacy(train, 100)\n",
    "# Save our trained Model\n",
    "modelfile = 'wtf_3_100_epoch'\n",
    "prdnlp.to_disk(modelfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Loaded meta.json from file\n",
      "wtf_2\\meta.json\n",
      "✔ Successfully created package 'en_model-0.0.0'\n",
      "packages\\en_model-0.0.0\n",
      "To build the package, run `python setup.py sdist` in this directory.\n"
     ]
    }
   ],
   "source": [
    "## save the trained pipeline into a .tar.gz package\n",
    "!python -m spacy package wtf_2 ./packages \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generation of a  config\n",
    "!python -m spacy init config ./config.cfg --lang en --pipeline ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import spacy\n",
    "# from spacy.tokens import Span\n",
    "# nlp = spacy.blank(\"en\")\n",
    "# doc3 = nlp(\"There's also a Paris in Arkansas, lol\")\n",
    "# for ent in doc3.ents:\n",
    "#     print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = TRAIN_DATA[int(len(TRAIN_DATA) *0.07):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test your text\n",
    "\n",
    "TESTDATA = result.values.tolist()\n",
    "TESTDATA = [(item[0], {'merchant': item[1], 'entities':[(item[2], item[3], 'BRD')]}) for item in TESTDATA]\n",
    "test = TESTDATA[int(len(TESTDATA) *0.07):]\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "test_text = sample(test, 1000)\n",
    "scores = []\n",
    "for text in test_text:\n",
    "    #print('test case: ', text)\n",
    "    doc = prdnlp(text[0])\n",
    "    for ent in doc.ents:\n",
    "        # print(ent.text)\n",
    "        # print('True label is: ')\n",
    "        # print(text[1]['merchant'])\n",
    "        similarity = fuzz.token_set_ratio(ent.text,text[1]['merchant'])\n",
    "        #print('similarity is: ', similarity)\n",
    "        scores.append(similarity)\n",
    "print(scores.count(100) / len(scores) *100, ' percent of test data has been perfectly correctly classified.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test case:  ('leycrest foodmart valero', {'merchant': 'valero', 'entities': [(18, 24, 'BRD')]})\n",
      "valero\n",
      "test case:  ('flo dollar general', {'merchant': 'dollar general', 'entities': [(4, 18, 'BRD')]})\n",
      "dollar general\n",
      "test case:  ('xfinity mi creek retail battle usa', {'merchant': 'xfinity', 'entities': [(0, 7, 'BRD')]})\n",
      "xfinity\n",
      "test case:  ('food lion 2312 sc columbia', {'merchant': 'food lion', 'entities': [(0, 9, 'BRD')]})\n",
      "food lion\n",
      "test case:  ('bp canaqps 14', {'merchant': 'bp', 'entities': [(0, 2, 'BRD')]})\n",
      "bp\n",
      "test case:  ('disc replay rockford', {'merchant': 'disc replay', 'entities': [(0, 11, 'BRD')]})\n",
      "disc replay\n",
      "test case:  ('ecomm 6161 dions 15 nmus 505', {'merchant': 'dions', 'entities': [(11, 16, 'BRD')]})\n",
      "dions\n",
      "test case:  ('best buy 0599 brooklyn ny usa', {'merchant': 'best buy', 'entities': [(0, 8, 'BRD')]})\n",
      "best buy\n",
      "test case:  ('kfc fl orlando usa', {'merchant': 'kfc', 'entities': [(0, 3, 'BRD')]})\n",
      "kfc\n",
      "test case:  (\"new sharks fish chishark's fish and chicken\", {'merchant': \"shark's fish and chicken\", 'entities': [(19, 43, 'BRD')]})\n"
     ]
    }
   ],
   "source": [
    "load_nlp = spacy.load('wtf_2')\n",
    "test_text = sample(test, 10)\n",
    "for text in test_text:\n",
    "    print('test case: ', text)\n",
    "    doc = load_nlp(text[0])\n",
    "    for ent in doc.ents:\n",
    "        print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brother s BRD\n"
     ]
    }
   ],
   "source": [
    "doc = load_nlp('brother s 3659 nebrask')\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>merchant_name</th>\n",
       "      <th>cleanName2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229237</th>\n",
       "      <td>n &amp; h liquor</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168976</th>\n",
       "      <td>jalao</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278372</th>\n",
       "      <td>paypal smith donny</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150446</th>\n",
       "      <td>heap cheap</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265328</th>\n",
       "      <td>paypal goodstart20 eb</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332356</th>\n",
       "      <td>school dist5 lex payroll</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145106</th>\n",
       "      <td>h &amp; s</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313795</th>\n",
       "      <td>remitly x52c seattle wa usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295420</th>\n",
       "      <td>pp facebookpay teamlillyf</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218989</th>\n",
       "      <td>mikes steak house and var</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443646</th>\n",
       "      <td>wild cat package store columbus ga usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46297</th>\n",
       "      <td>brother s 3659 nebrask</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3507</th>\n",
       "      <td>281 saloon</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174296</th>\n",
       "      <td>johnnys a street m</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>427698</th>\n",
       "      <td>usa medical center</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10982</th>\n",
       "      <td>a0000000000aq6c save a stop memphis tnus</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415877</th>\n",
       "      <td>tst 1428 haight patio</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277341</th>\n",
       "      <td>paypal secretnaila</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275527</th>\n",
       "      <td>paypal psychicelai</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270568</th>\n",
       "      <td>paypal lisa rose</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55824</th>\n",
       "      <td>cash max</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360638</th>\n",
       "      <td>sq biscuitville 127 burlington nc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>450011</th>\n",
       "      <td>y camp truck sto</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208817</th>\n",
       "      <td>market square beverage</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195837</th>\n",
       "      <td>lindsey sisneros</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414722</th>\n",
       "      <td>triple r auto sales</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73191</th>\n",
       "      <td>contacts subscription</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255646</th>\n",
       "      <td>paul mitchell the schoolgrand rapids mi usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>309964</th>\n",
       "      <td>remitly a901 seattle wa usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223029</th>\n",
       "      <td>moisture pop</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59386</th>\n",
       "      <td>champion communications</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275762</th>\n",
       "      <td>paypal qyadeara</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223982</th>\n",
       "      <td>montevallo shell sneaky p</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71963</th>\n",
       "      <td>commerce vending inc</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115999</th>\n",
       "      <td>fishermans cove</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228362</th>\n",
       "      <td>mx sports inc 13042840101 wv usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34245</th>\n",
       "      <td>bell road</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353964</th>\n",
       "      <td>sp prettigirlstore</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354758</th>\n",
       "      <td>sp southern shirt tuscaloosa al usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>375560</th>\n",
       "      <td>sq tasty snow</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233654</th>\n",
       "      <td>new belgium brewery</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69554</th>\n",
       "      <td>cm school supply riverside ca usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42927</th>\n",
       "      <td>boston commons</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46796</th>\n",
       "      <td>bruces greenhouse llc hamilton oh usa</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174450</th>\n",
       "      <td>johns place</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243624</th>\n",
       "      <td>ol westminser direct dep</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370835</th>\n",
       "      <td>sq nothing better</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228243</th>\n",
       "      <td>mva essex dlps</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75287</th>\n",
       "      <td>country style catering</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20783</th>\n",
       "      <td>amk unlv coffee bn tea las vegas nv</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      merchant_name cleanName2\n",
       "229237                                 n & h liquor        NaN\n",
       "168976                                        jalao        NaN\n",
       "278372                           paypal smith donny        NaN\n",
       "150446                                   heap cheap        NaN\n",
       "265328                        paypal goodstart20 eb        NaN\n",
       "332356                     school dist5 lex payroll        NaN\n",
       "145106                                        h & s        NaN\n",
       "313795                  remitly x52c seattle wa usa        NaN\n",
       "295420                    pp facebookpay teamlillyf        NaN\n",
       "218989                    mikes steak house and var        NaN\n",
       "443646       wild cat package store columbus ga usa        NaN\n",
       "46297                        brother s 3659 nebrask        NaN\n",
       "3507                                     281 saloon        NaN\n",
       "174296                           johnnys a street m        NaN\n",
       "427698                           usa medical center        NaN\n",
       "10982      a0000000000aq6c save a stop memphis tnus        NaN\n",
       "415877                        tst 1428 haight patio        NaN\n",
       "277341                           paypal secretnaila        NaN\n",
       "275527                           paypal psychicelai        NaN\n",
       "270568                             paypal lisa rose        NaN\n",
       "55824                                      cash max        NaN\n",
       "360638            sq biscuitville 127 burlington nc        NaN\n",
       "450011                             y camp truck sto        NaN\n",
       "208817                       market square beverage        NaN\n",
       "195837                             lindsey sisneros        NaN\n",
       "414722                          triple r auto sales        NaN\n",
       "73191                         contacts subscription        NaN\n",
       "255646  paul mitchell the schoolgrand rapids mi usa        NaN\n",
       "309964                  remitly a901 seattle wa usa        NaN\n",
       "223029                                 moisture pop        NaN\n",
       "59386                       champion communications        NaN\n",
       "275762                              paypal qyadeara        NaN\n",
       "223982                    montevallo shell sneaky p        NaN\n",
       "71963                          commerce vending inc        NaN\n",
       "115999                              fishermans cove        NaN\n",
       "228362             mx sports inc 13042840101 wv usa        NaN\n",
       "34245                                     bell road        NaN\n",
       "353964                           sp prettigirlstore        NaN\n",
       "354758          sp southern shirt tuscaloosa al usa        NaN\n",
       "375560                                sq tasty snow        NaN\n",
       "233654                          new belgium brewery        NaN\n",
       "69554             cm school supply riverside ca usa        NaN\n",
       "42927                                boston commons        NaN\n",
       "46796         bruces greenhouse llc hamilton oh usa        NaN\n",
       "174450                                  johns place        NaN\n",
       "243624                     ol westminser direct dep        NaN\n",
       "370835                            sq nothing better        NaN\n",
       "228243                               mva essex dlps        NaN\n",
       "75287                        country style catering        NaN\n",
       "20783           amk unlv coffee bn tea las vegas nv        NaN"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tempdata = pd.read_csv('data/cleanedmerchant_training.csv') ## read the file into to a pandas dataframe\n",
    "data = tempdata[['merchant_name', 'cleanName2']]\n",
    "invalidName = data[data['cleanName2'].isnull()]\n",
    "invalidName.reset_index(inplace=True, drop = True)\n",
    "invalidName.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# mu, sigma = 100, 10 # mean and standard deviation\n",
    "# s = np.random.normal(mu, sigma, 1000)\n",
    "# s_2 = np.random.uniform(70, 130, 1000)\n",
    "\n",
    "# count, bins, ignored = plt.hist(s, 30, density=False, color = 'r')\n",
    "# # plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) *\n",
    "# #                np.exp( - (bins - mu)**2 / (2 * sigma**2) )\n",
    "# #          )\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_docs = [\"Here are some very simple basic sentences.\",\n",
    "# \"They won't be very interesting, I'm afraid.\",\n",
    "# \"The point of these examples is to _learn how basic text cleaning works_ on *very simple* data.\"]\n",
    "raw_docs = validName['merchant_name'].to_list()\n",
    "tokenized_docs = [word_tokenize(doc) for doc in raw_docs]\n",
    "print(tokenized_docs[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "regex = re.compile('[%s]' % re.escape(string.punctuation)) #see documentation here: http://docs.python.org/2/library/string.html\n",
    "\n",
    "tokenized_docs_no_punctuation = []\n",
    "\n",
    "for review in tokenized_docs:\n",
    "    new_review = []\n",
    "    for token in review:\n",
    "        new_token = regex.sub(u'', token)\n",
    "        if not new_token == u'':\n",
    "            new_review.append(new_token)\n",
    "    \n",
    "    tokenized_docs_no_punctuation.append(new_review)\n",
    "    \n",
    "print(tokenized_docs_no_punctuation[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenized_docs_no_stopwords = []\n",
    "\n",
    "for doc in tokenized_docs_no_punctuation:\n",
    "    new_term_vector = []\n",
    "    for word in doc:\n",
    "        if not word in stopwords.words('english'):\n",
    "            new_term_vector.append(word)\n",
    "    \n",
    "    tokenized_docs_no_stopwords.append(new_term_vector)\n",
    "\n",
    "print(tokenized_docs_no_stopwords[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [' '.join(words) for words in tokenized_docs_no_stopwords]\n",
    "len(cleaned_sentences) == len(validName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName['merchant_name'] = cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_dataset(dataset):\n",
    "    \"\"\"\n",
    "    Shuffle samples in dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset: (list) list of tuples like: [(transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    Returns:\n",
    "        dataset: (list) list of tuples like: [,....., (transaction, start_idx, end_idx), ..... , ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    shuffled_indices = list(range(len(dataset)))\n",
    "    random.shuffle(shuffled_indices)\n",
    "    dataset = [dataset[index] for index in shuffled_indices]\n",
    "\n",
    "    return dataset\n",
    "def get_merchant_indices_in_sentence(sentence, merchant):\n",
    "    \"\"\"\n",
    "    Given transaction string and merchant string, returns start end end indices of merchant in transaction string.\n",
    "\n",
    "    Args:\n",
    "        sentence: (string) transaction string. (example: )\n",
    "        merchant: (string) merchant name. (example: )\n",
    "\n",
    "    Returns:\n",
    "        sentence: (string) converted to upper strings\n",
    "        start: (int) merchant string start index\n",
    "        end: (int) merchant string end index\n",
    "\n",
    "    Examples:\n",
    "        sentences, start, end = get_sentence_indices(\"Target 00014423 WATERTOWN MA\",\"target\")\n",
    "\n",
    "        sentences: \"TARGET 00014423 WATERTOWN MA\"\n",
    "        start: 0\n",
    "        end: 6\n",
    "    \"\"\"\n",
    "\n",
    "    sentence = sentence.upper()\n",
    "    merchant = merchant.upper()\n",
    "\n",
    "    start = -1\n",
    "    end = -1\n",
    "\n",
    "    idx = sentence.find(merchant)\n",
    "    if idx != -1:\n",
    "        start = idx\n",
    "        end = start + len(merchant)\n",
    "\n",
    "    return sentence, start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "for idx, row in validName.iterrows():\n",
    "    # print('Processing row: ', idx)\n",
    "    # print('row0: ', row[0])\n",
    "    # print('row1: ', row[1])\n",
    "    # print(row[0].find(row[1].split(' ')[0]))\n",
    "    # print(len(row[1]))\n",
    "    find_i = row[0].find(row[1].split(' ')[0])\n",
    "    if find_i + len(row[1]) > len(row[0]):   \n",
    "        row[0] = row[0][:find_i] + row[1]\n",
    "    \n",
    "    sentence, start, end = get_merchant_indices_in_sentence(row[0], row[1])\n",
    "    validName.loc[idx, 'start_idx'] = start\n",
    "    validName.loc[idx, 'end_idx'] = end\n",
    "\n",
    "print(validName.sample(50))\n",
    "#validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validName.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset.drop(columns = 'Unnamed: 0')\n",
    "cleandataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset = cleandataset[cleandataset['start_idx']!=-1]\n",
    "len(cleandataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandataset.to_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_embedding(text, max_len=300, emb_dim=8):\n",
    "        \"\"\"\n",
    "        Embeds character string with the use of (emb_dim)-bit binary values of each character.\n",
    "\n",
    "        Args:\n",
    "            text: (string) text to embed\n",
    "            max_len: (int) maximum length of text that will be encoded. Padding will be done with zeros.\n",
    "            emb_dim:\n",
    "\n",
    "        Returns:\n",
    "            str_array: (ndarray) 2 dimensional numpy array containing embedded text of shape emb_dim*max_len\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        # cut long text with maximum accepted length\n",
    "        if len(text) > max_len:\n",
    "            text = text[:max_len]\n",
    "\n",
    "        str_array = np.zeros((emb_dim, max(len(text), max_len)), dtype=np.int32).tolist()\n",
    "\n",
    "        for index, char in enumerate(text):\n",
    "            str_binary = format(ord(char), 'b').zfill(emb_dim)[::-1]\n",
    "            str_binary = str_binary[:emb_dim]\n",
    "            for str_index, str_char in enumerate(str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "       \n",
    "        padding_str_binary = '0' * emb_dim\n",
    "        \n",
    "        for index in range(len(text), max_len):\n",
    "            for str_index, str_char in enumerate(padding_str_binary, 0):\n",
    "                str_array[str_index][index] = int(str_char)\n",
    "\n",
    "        return str_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "emb = character_embedding('Martin')\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = cleandataset.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = df[:int(0.7 * len(df))]\n",
    "val_dataset = df[int(0.7 * len(df)): int(0.85 * len(df))]\n",
    "test_dataset = df[int(0.85 * len(df)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rule matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"sID\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"dollar general w main a\")\n",
    "print(doc.ents)\n",
    "# for idx, row in sample_10.iterrows():\n",
    "#     # print('processing row: ', idx)\n",
    "#     # print(row[0])\n",
    "#     doc = nlp(row[0])\n",
    "#     all_entities = doc.ents\n",
    "#     print(all_entities)\n",
    "#     # print(all_entities)\n",
    "#     #extraction_temp = str(process.extractOne(row[1], all_entities)[0])\n",
    "#     #extraction =  re.sub(r\"inc|mktp|\\d+\", \"\", extraction_temp) \n",
    "#     #print('extraction result: ', extraction_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "cleanData = pd.read_csv('data/cleanedmerchant_training.csv')\n",
    "cleanData.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwordFreeds[(stopwordFreeds['cleanName2'] != stopwordFreeds['CleanName3']) & (stopwordFreeds['CleanName3'] != 'chick fil')].sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5109d816b82be14675a6b11f8e0f0d2e80f029176ed3710d54e125caa8520dfd"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
